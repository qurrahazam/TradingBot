# -*- coding: utf-8 -*-
"""Modified.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ijKRZrTDPgMWwwjBrguBpnU0JPDR-eRp
"""

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import gym
from gym import spaces
from stable_baselines3 import PPO
import numpy as np
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.callbacks import BaseCallback
import matplotlib.pyplot as plt

"""***Data Normalization***

---
"""

def normalize_data(data,  price_features, scaler=None):
    data_scaled = data.copy()
    if scaler is None:
        scaler = MinMaxScaler()
    data_scaled[price_features] = scaler.fit_transform(data_scaled[price_features])
    data_scaled.rename(columns={col: f"{col}" for col in price_features}, inplace=True)
    return data_scaled

"""***Steps to mitigate overfitting***

---


"""

# Define EarlyStoppingCallback
class EarlyStoppingCallback(BaseCallback):
    def __init__(self, eval_env, model, patience=10, save_path=None):
        super().__init__()
        self.eval_env = eval_env
        self.model = model  # Pass the model instance
        self.patience = patience
        self.save_path = save_path
        self.best_mean_reward = -float('inf')
        self.steps_since_best_reward = 0

    def _on_step(self) -> bool:
        # Evaluate the model on the validation environment
        mean_reward, _ = evaluate_policy(self.model, self.eval_env)

        # Check if the mean reward has improved
        if mean_reward > self.best_mean_reward:
            self.best_mean_reward = mean_reward
            self.steps_since_best_reward = 0
            if self.save_path is not None:
                self.model.save(self.save_path)  # Save the best model
        else:
            self.steps_since_best_reward += 1

        # Check if patience has been exceeded
        if self.steps_since_best_reward > self.patience:
            print("Early stopping: Validation reward hasn't improved for {} steps".format(self.patience))
            return False  # Stop training

        return True  # Continue training

"""***Environment for agent to interact***

---


"""

class TradingEnv(gym.Env):
    action_mapping = {
        0: "hold",
        1: "buy",
        2: "sell"
    }

    reward_mapping = {
        False: {
            "hold": "no_trade_hold",
            "buy": "no_trade_buy",
            "sell": "no_trade_sell"
        },
        True: {
            "hold": "in_trade_hold",
            "buy": "in_trade_buy",
            "sell": "in_trade_sell"
        }
    }

    def __init__(self, data_df, features=None ,initial_capital=0):
        self.window_size = 5
        self.data_df = data_df.copy()  # Avoid modifying original data
        self.initial_capital = initial_capital
        self.action_space = gym.spaces.Discrete(3)  # Buy, Sell, Hold (3 actions)
        self.features = features if features is not None else []
        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(self.window_size + 1 + len(self.features),))
        self.price_data = data_df[[self.features[0], self.features[1]]].copy()


    def reset(self):
        # Resets the environment to the beginning of a new episode.
        self.current_step = 0
        self.capital = self.initial_capital
        self.in_trade = False  # Track if currently holding an asset
        self.observation = self._get_observation()
        return self.observation

    def step(self, action):
        # Determine reward based on trade status (in_trade or no_trade) and action
        action_string = TradingEnv.action_mapping[int(action)]
        reward_column = TradingEnv.reward_mapping[self.in_trade][action_string]
        reward = self.data_df[reward_column].iloc[self.current_step]

        if not self.in_trade:  # Not currently in a trade (no_trade)
            if action_string == "hold":  # Buy (no_trade)
                self.capital += reward  # Because it is a valid action
            elif action_string == "buy":  # Sell (no_trade) - penalty for invalid action
                self.capital += reward  # Because it is a valid action
                self.in_trade = True  # Enter trade after buying
            elif action_string == "sell":  # Invalid action
                pass

        elif self.in_trade:
            if action_string == "hold":
                self.capital += reward  # Because it is a valid action
            elif action_string == "buy":  # Invalid action (already in trade)
                pass
            elif action_string == "sell":
                self.capital += reward  # Because it is a valid action
                self.in_trade = False  # Exit trade after selling

        self.current_step += 1
        self.observation = self._get_observation()

        done = self.current_step >= len(self.data_df) - 1  # Done at the end of data

        info = {
            'current_step': self.current_step,
            'action_string': action_string,
            'in_trade': self.in_trade,
            'capital': self.capital,
        }  # Additional information

        return self.observation, reward, done, info

    def _get_observation(self):
        if self.current_step < self.window_size:
            # If current_step is less than the window size, pad with zeros
            past_prices = self.data_df[self.features[0]].iloc[:self.current_step + 1]
            padded_prices = np.pad(past_prices.to_numpy(), (self.window_size - len(past_prices), 0), mode='constant')
        else:
            # Extract past prices with the correct window size
            past_prices = self.data_df[self.features[0]].iloc[
                          self.current_step - self.window_size + 1: self.current_step + 1]
            padded_prices = past_prices.to_numpy()

        # Initialize the observation with padded prices
        observation = padded_prices

        # Include additional features if available
        for feature in self.features:
            if feature in self.data_df.columns:
                observation = np.append(observation, self.data_df[feature].iloc[self.current_step - self.window_size +1 ])
            else:
                # If the feature is not available, pad with zeros
                observation = np.append(observation, 0)

        # Append the trade status to the observation
        trade_status = 1 if self.in_trade else 0
        observation_with_status = np.append(observation, trade_status)

        return observation_with_status

from google.colab import drive
drive.mount('/content/drive')

features = ['ask_price', 'bid_price','bu01', 'bu02', 'bu03', 'bu04', 'bu05',
       'bu06', 'bu07', 'bu08', 'bu09', 'bu10', 'bu11', 'bu12', 'bu13', 'bu14',
       'bu15', 'bu16', 'bu17', 'bu18', 'bu19', 'bu20']

"""***Loading Data***

---


"""

train_data_o = pd.read_parquet("/content/drive/MyDrive/train.gzip")
# This is just for experiment purpose increase the dataset of apply the model on full up to you
tdata = train_data_o.head(10)
train_data = normalize_data(tdata, features)
val_data_o= pd.read_parquet("/content/drive/MyDrive/validation.gzip")
vdata = val_data_o.head(10)
val_data = normalize_data(vdata, features)

# Example usage (assuming your data is loaded into a pandas DataFrame called 'train_data_o')
env = TradingEnv(train_data_o.copy(), features, initial_capital=5000)  # Set initial capital to 5000
observation = env.reset()

# Interact with the environment by taking actions and observing rewards
for _ in range(100):
    action = env.action_space.sample()
    print(action)  # Sample a random action
    observation, reward, done, info = env.step(action)
    # Process the observation, reward, and info
    trade_status = "in_trade" if info['in_trade'] else "no_trade"
    print(f"Action: {action}, Reward: {reward}, Capital: {info['capital']}, Trade Status: {trade_status}")
    if done:
        break

env.close()

"""***Training Loop***

---

"""

epsilon_initial = 1.0
epsilon_min = 0.1
# slow down the decay rate so there is a balanced trade off between exploitation and exploration
epsilon_decay = 0.1
# increase the number of episodes for better results
num_episodes = 3
eval_freq = 10
eval_episodes = 5

# print(train_data)

# Assuming your environment is defined in 'TradingEnv' class
env = TradingEnv(train_data.copy(), features)  # Create an environment instance
model = PPO(policy='MlpPolicy', env=env, learning_rate=0.001, gamma=0.9, verbose=1)

val_env = TradingEnv(val_data.copy(), features)  # Create an environment instance
val_env = DummyVecEnv([lambda: val_env])  # TODO: Why redefine 'val_env' ?  This approach allows you to leverage the benefits of vectorized environment. Now you can ask thwn why not Env? For computational and compatibilty reasons.

# Create an instance of EarlyStoppingCallback
early_stopping_callback = EarlyStoppingCallback(eval_env=val_env, model=model, patience=10, save_path="best_model.zip")

# Training loop with early stopping
for episode in range(num_episodes):
    # Calculate current exploration parameter
    epsilon = max(epsilon_initial - episode * epsilon_decay, epsilon_min)
    # print(epsilon)

    # Reset the environment
    observation = env.reset()

    # Interact with the environment
    x = len(train_data)
    while x > 0:
        # Take action using Îµ-greedy strategy
        if np.random.rand() < epsilon:
            # Random action (explore)
            action = env.action_space.sample()
            # print("Random Action:", action)
        else:
            # Action with highest expected return (exploit)
            action, _ = model.predict(observation)
            # print("Predicted Action:", action)

        # Perform action in the environment
        next_observation, reward, done, info = env.step(action)

        # Update model with collected experience
        model.learn(total_timesteps=1)

        # Update observation
        observation = next_observation

        x -= 1

    # Evaluate the model periodically (if needed)
    if episode % eval_freq == 0:
        # Evaluate the model
        mean_reward, _ = evaluate_policy(model, val_env, n_eval_episodes=eval_episodes)
        print(f"Episode: {episode}, Mean Reward: {mean_reward}")

    # Check for early stopping
    if not early_stopping_callback._on_step():
        print("Early stopping triggered.")
        break

"""***BackTesting***

---


"""

def backtest_and_plot(env, model, num_episodes=1000):
    total_profit = 0
    total_reward = 0
    average_holding_period = 0
    win_count = 0

    rewards_per_episode = []  # List to store rewards for each episode
    actions_per_episode = []  # List to store actions for each episode

    for _ in range(num_episodes):
        observation = env.reset()
        done = False
        holding_period = 0
        initial_capital = env.envs[0].capital  # Accessing capital attribute from the underlying environment
        episode_actions = []  # List to store actions for the current episode

        while not done:
            action, _ = model.predict(observation)
            observation, reward, done, info = env.step(action)

            total_reward += reward
            holding_period += 1
            episode_actions.append(action)  # Store the action taken at each step

            # Check if a trade was closed (sold) and if it resulted in a profit
            if 'in_trade' in info and not info['in_trade'] and reward > 0:
                win_count += 1

        episode_profit = env.envs[0].capital - initial_capital  # Calculate episode profit from the underlying environment
        total_profit += episode_profit
        average_holding_period += holding_period / (holding_period + 1)

        rewards_per_episode.append(total_reward)  # Store total reward for the episode
        actions_per_episode.append(episode_actions)  # Store actions for the episode

    average_profit = total_profit / num_episodes
    average_reward = np.mean(total_reward)  # Calculate mean reward using np.mean()
    average_holding_period /= num_episodes
    win_rate = win_count / num_episodes

    print("Backtesting Results:")
    print(f"Average Profit: {average_profit:.2f}")
    print(f"Average Reward: {average_reward:.2f}")  # Use np.mean() result
    print(f"Average Holding Period: {average_holding_period:.2f} steps")
    print(f"Win Rate: {win_rate:.2%}")

    # Plot rewards per episode
    plt.figure(figsize=(10, 6))
    plt.plot(rewards_per_episode)
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')
    plt.title('Total Reward per Episode')
    plt.grid(True)
    plt.show()

    return actions_per_episode  # Return actions taken for each episode

# Load the trained model
model = PPO.load("best_model.zip")

# Load the test dataset
test_data_o = pd.read_parquet("/content/drive/MyDrive/test.gzip")
test_data = normalize_data(test_data_o.head(1000), features)  # Assuming you want to use only the first 1000 rows for testing; Can chnage it to test on full data set

# Create a trading environment for testing
test_env = TradingEnv(test_data.copy(), features)
test_env = DummyVecEnv([lambda: test_env])

# Evaluate the model on the test environment
actions = backtest_and_plot(test_env, model, num_episodes=2)

def plot_trades(actions_per_episode, test_data):
    price_data = test_data[['ask_price', 'bid_price']].copy()

    # Combine actions from all episodes into a single array
    all_actions = [action for episode_actions in actions_per_episode for action in episode_actions]

    buy_indices = [i for i, action in enumerate(all_actions) if action == 2]  # Find indices of buy actions
    sell_indices = [i for i, action in enumerate(all_actions) if action == 1]  # Find indices of sell actions

    # Ensure that buy and sell indices are within the range of price_data indices
    buy_indices = [idx for idx in buy_indices if idx < len(price_data)]
    sell_indices = [idx for idx in sell_indices if idx < len(price_data)]

    buy_signals = price_data.iloc[buy_indices]
    sell_signals = price_data.iloc[sell_indices]

    plt.figure(figsize=(14, 7))
    plt.plot(price_data.index, price_data['ask_price'], label='Ask Price', alpha=0.5)
    plt.plot(price_data.index, price_data['bid_price'], label='Bid Price', alpha=0.5)
    plt.scatter(buy_signals.index, buy_signals['ask_price'], label='Buy', marker='^', color='green', alpha=1)
    plt.scatter(sell_signals.index, sell_signals['bid_price'], label='Sell', marker='v', color='red', alpha=1)
    plt.title('Trading signals')
    plt.xlabel('Step')
    plt.ylabel('Price')
    plt.legend()
    plt.show()

plot_trades(actions, test_data)

